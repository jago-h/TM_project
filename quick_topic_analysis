import pandas as pd
from collections import Counter
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# Load the dataset from the CSV file
df = pd.read_csv('super_clean_df_with_sentiment.csv')

# Get the negative reviews with Polarity determined by text blog as negative
negative_reviews = df[df['Sentiment Polarity'] < 0]['text']

# Create a CountVectorizer object
vectorizer = CountVectorizer(stop_words='english', max_features=70)

# Fit and transform the text data
X = vectorizer.fit_transform(negative_reviews)

# Get the most important words for each topic
feature_names = vectorizer.get_feature_names_out()

# Instantiate the LDA model
"""
topic_word_prior increased from 1.0 to 1.5 for a bigger spread of topics,
doc_topic_prior decreased from 1.0 to 0.2 for a more precise focus on the dominant words in those topics
"""
lda = LatentDirichletAllocation(n_components=5, topic_word_prior=1.5, doc_topic_prior=0.2, random_state=1)

# Fit the LDA model to the transformed data
lda.fit(X)

# Get the most important two-word combinations for each topic
num_top_combinations = 5  # Number of top two-word combinations to display for each topic

for topic_idx, topic in enumerate(lda.components_):
    top_indices = topic.argsort()[:-num_top_combinations - 1:-1]
    top_combinations = [feature_names[i-1] + ' ' + feature_names[i] for i in top_indices]
    print(f'Topic #{topic_idx + 1}:')
    print(', '.join(top_combinations))
    print()

# Get the dominant topic for each review
dominant_topics = lda.transform(X)
dominant_topic_indices = dominant_topics.argmax(axis=1)

# Count the occurrences of each dominant topic
topic_counter = Counter(dominant_topic_indices)

# Get the most common topics and their frequencies
most_common_topics = topic_counter.most_common(5)

# Print the most common topics and their frequencies
for topic, count in most_common_topics:
    print(f'Topic #{topic + 1}: {count} reviews')
